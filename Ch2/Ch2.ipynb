{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import standard packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set-Up of Matrices for Collaborative filtering\n",
    "\n",
    "First we want to implement the ratings matrix from Table 2.1 featuring 5 users and 6 items. We want to employ a simple user-based and item-based model for imputing ratings. For that we will use the cosine and the pearson-correlation-coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables 2.1 and 2.2\n",
    "\n",
    "table_2_1 = np.array([[7, 6, 7, 4, 5 , 4], [6,7,np.nan, 4,3,4], [np.nan, 3, 3, 1, 1, np.nan], [1, 2, 2 , 3 ,3 , 4], [1, np.nan, 1, 2, 3, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to compute the pearson-correlation-coefficient. The formula for this is:\n",
    "\n",
    "$ \\mu_u = \\frac{\\sum_{k \\in I_{u}} r_{uk}}{|I_{u}|} $\n",
    "\n",
    "$  Sim(u,v) = Pearson(u,v) = \\frac{\\sum_{k \\in I_{u} \\cap I_{v}} (r_{uk}- \\mu_{u}) \\cdot (r_{uk}- \\mu_{u}) }{\\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} (r_{uk}- \\mu_{u})^2} \\cdot \\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} (r_{vk}- \\mu_{v})^2}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(table, x: int, y: int):\n",
    "    # x and y are the rows of table on which to perform computation\n",
    "    # get I_x and I_y\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    I_y = np.where(np.isnan(table[y]) == False)[0]\n",
    "    # get the common indices\n",
    "    I = np.intersect1d(I_x, I_y)\n",
    "    # get the mean of x and y\n",
    "    x_mean = np.mean(table[x][I])\n",
    "    y_mean = np.mean(table[y][I])\n",
    "    # get the numerator\n",
    "    numerator = np.sum((table[x][I] - x_mean)*(table[y][I] - y_mean))\n",
    "    # get the denominator\n",
    "    denominator = np.sqrt(np.sum((table[x][I] - x_mean)**2)*np.sum((table[y][I] - y_mean)**2))\n",
    "    # get the correlation\n",
    "    corr = numerator/denominator\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the correlation coefficient for rows 0 and 2 (1 and 3 in the book). In the book, it is given as 0.894."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8944271909999159)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_corr(table_2_1, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, we can also compute the entire correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "       [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "       [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "       [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "       [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a correlation matrix for the table\n",
    "corr_matrix = np.zeros((5,5))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        corr_matrix[i,j] = pearson_corr(table_2_1, i, j)\n",
    "\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix_func(table, dimension):\n",
    "    if dimension == 0:\n",
    "        number = table.shape[dimension]\n",
    "        corr_matrix = np.zeros((number, number))\n",
    "        for i in range(number):\n",
    "            for j in range(number):\n",
    "                corr_matrix[i,j] = pearson_corr(table, i, j)\n",
    "    elif dimension == 1:\n",
    "        number = table.shape[dimension]\n",
    "        corr_matrix = np.zeros((number, number))\n",
    "        table = table.T\n",
    "        for i in range(number):\n",
    "            for j in range(number):\n",
    "                corr_matrix[i,j] = pearson_corr(table, i, j)\n",
    "    else:\n",
    "        print(\"Invalid dimension\")\n",
    "        return None\n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "       [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "       [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "       [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "       [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix_func(table = table_2_1, dimension = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maggo\\AppData\\Local\\Temp\\ipykernel_4888\\2376229201.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  corr = numerator/denominator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "        [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "        [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "        [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "        [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]]),\n",
       " array([[1.        , 0.94063416, 0.98782916, 0.89714996, 0.67675297,\n",
       "         0.57263713],\n",
       "        [0.94063416, 1.        , 0.99862543, 0.69310328, 0.51449576,\n",
       "                nan],\n",
       "        [0.98782916, 0.99862543, 1.        , 0.6381449 , 0.62092042,\n",
       "         0.62861856],\n",
       "        [0.89714996, 0.69310328, 0.6381449 , 1.        , 0.81348922,\n",
       "         0.87038828],\n",
       "        [0.67675297, 0.51449576, 0.62092042, 0.81348922, 1.        ,\n",
       "         0.33333333],\n",
       "        [0.57263713,        nan, 0.62861856, 0.87038828, 0.33333333,\n",
       "         1.        ]]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix_func(table_2_1, 0), corr_matrix_func(table_2_1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we can compute the correlation coefficient both for rows or columns.\n",
    "\n",
    "Next, we can demean the rating in every line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demean_func(table):\n",
    "    # demean the table\n",
    "    new_table = np.zeros(table.shape)\n",
    "    for i in range(table.shape[0]):\n",
    "        I = np.where(np.isnan(table[i]) == False)[0]\n",
    "        new_table[i][I] = table[i][I] - np.mean(table[i][I])\n",
    "    return new_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5,  0.5,  1.5, -1.5, -0.5, -1.5],\n",
       "       [ 1.2,  2.2,  0. , -0.8, -1.8, -0.8],\n",
       "       [ 0. ,  1. ,  1. , -1. , -1. ,  0. ],\n",
       "       [-1.5, -0.5, -0.5,  0.5,  0.5,  1.5],\n",
       "       [-1. ,  0. , -1. ,  0. ,  1. ,  1. ]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_2 = demean_func(table_2_1)\n",
    "table_2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to store the means somewhere. Let us write a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.5, 4.8, 2. , 2.5, 2. ])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_compute(table, dimension = 0):\n",
    "    # compute the mean of the table\n",
    "    if dimension == 1:\n",
    "        table_copy = table.T\n",
    "    else:\n",
    "        table_copy = table\n",
    "    mean = np.zeros(table_copy.shape[0])\n",
    "    for i in range(table_copy.shape[0]):\n",
    "        I = np.where(np.isnan(table_copy[i]) == False)[0]\n",
    "        mean[i] = np.mean(table_copy[i][I])\n",
    "    return mean\n",
    "\n",
    "user_mean = mean_compute(table_2_1, 0)\n",
    "user_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.75, 4.5 , 3.25, 2.8 , 3.  , 3.75])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_compute(table_2_1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the demeaned table to better predict a rating. Suppose that we want to predict for user in 3 (row-index 2) their ratings for movies 1 and 6 (indices 0 and 5, respectively). We can first compute the set of relevant neighbours as those whose correlation exceeds 0.7 (as an arbitrary benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "       [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "       [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "       [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "       [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix_table_2_2 = corr_matrix_func(table_2_1, 0)\n",
    "corr_matrix_table_2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from themselves, this is the case for users 1 and 2 (indices 0 and 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = np.where((corr_matrix_table_2_2[2] > 0.7))\n",
    "k = k[0][:-1]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "        4.        ],\n",
       "       [6.        , 7.        ,        nan, 4.        , 3.        ,\n",
       "        4.        ],\n",
       "       [3.34386392, 3.        , 3.        , 1.        , 1.        ,\n",
       "               nan],\n",
       "       [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "        4.        ],\n",
       "       [1.        ,        nan, 1.        , 2.        , 3.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a deep copy of table_2_2\n",
    "table_2_1_copy = table_2_1.copy()\n",
    "table_2_1_copy[2,0] = user_mean[2] + np.sum(corr_matrix_table_2_2[2][k]*table_2_2[k,0])/np.sum(np.abs(corr_matrix_table_2_2[2][k]))\n",
    "table_2_1_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us encapsulate this in a function. We will have to specify a correlation-threshold to determine k, and then within the function we create a deep-copy that fills in the NaN-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_function(table, threshold, dimension):\n",
    "    table_copy = table.copy()\n",
    "    # compute correlation_matrix\n",
    "    corr_matrix = corr_matrix_func(table, dimension)\n",
    "    # compute means\n",
    "    user_mean = mean_compute(table, dimension)\n",
    "    # demean the table\n",
    "    table_copy_dm = demean_func(table_copy)\n",
    "    # for each row in correlation matrix compute the k based on threshold\n",
    "    k = [[] for _ in range(corr_matrix.shape[0])]\n",
    "    for i in range(corr_matrix.shape[0]):\n",
    "        k[i] = np.where((corr_matrix[i] > threshold))[0]\n",
    "    # for each value of k, remove the same number if it is in k, e.g. remove 2 from k[2]\n",
    "    for i in range(corr_matrix.shape[0]):\n",
    "        k[i] = np.setdiff1d(k[i], i)\n",
    "    # for each row and column in the table, compute the imputation\n",
    "    for i in range(table.shape[0]):\n",
    "        for j in range(table.shape[1]):\n",
    "            if np.isnan(table[i,j]):\n",
    "                table_copy[i,j] = user_mean[i] + np.sum(corr_matrix[i][k[i]]*table_copy_dm[k[i],j])/np.sum(np.abs(corr_matrix[i][k[i]]))\n",
    "    return table_copy, corr_matrix, user_mean, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2_1_imputed, corr_matrix_comp, user_mean_comp, k_comp = imputation_function(table_2_1, 0.7, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "        4.        ],\n",
       "       [6.        , 7.        , 6.0135157 , 4.        , 3.        ,\n",
       "        4.        ],\n",
       "       [3.34386392, 3.        , 3.        , 1.        , 1.        ,\n",
       "        0.86431752],\n",
       "       [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "        4.        ],\n",
       "       [1.        , 1.5       , 1.        , 2.        , 3.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "       [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "       [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "       [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "       [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.5, 4.8, 2. , 2.5, 2. ])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_mean_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2]), array([0, 2]), array([0, 1]), array([4]), array([3])]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interestingly enough, we can do the entire thing now as well for items!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maggo\\AppData\\Local\\Temp\\ipykernel_4888\\2376229201.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  corr = numerator/denominator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "        4.        ],\n",
       "       [6.        , 7.        , 5.74252405, 4.        , 3.        ,\n",
       "        4.        ],\n",
       "       [4.59918476, 3.        , 3.        , 1.        , 1.        ,\n",
       "        2.10190223],\n",
       "       [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "        4.        ],\n",
       "       [1.        , 2.5       , 1.        , 2.        , 3.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1_imputed_cols, corr_matrix_comp_cols, user_mean_comp_cols, k_comp_cols = imputation_function(table_2_1, 0.7, 1)\n",
    "table_2_1_imputed_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.94063416, 0.98782916, 0.89714996, 0.67675297,\n",
       "        0.57263713],\n",
       "       [0.94063416, 1.        , 0.99862543, 0.69310328, 0.51449576,\n",
       "               nan],\n",
       "       [0.98782916, 0.99862543, 1.        , 0.6381449 , 0.62092042,\n",
       "        0.62861856],\n",
       "       [0.89714996, 0.69310328, 0.6381449 , 1.        , 0.81348922,\n",
       "        0.87038828],\n",
       "       [0.67675297, 0.51449576, 0.62092042, 0.81348922, 1.        ,\n",
       "        0.33333333],\n",
       "       [0.57263713,        nan, 0.62861856, 0.87038828, 0.33333333,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix_comp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3]),\n",
       " array([0, 2]),\n",
       " array([0, 1]),\n",
       " array([0, 4, 5]),\n",
       " array([3]),\n",
       " array([3])]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_comp_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.1 Similarity Function Variants\n",
    "\n",
    "We have now computed using the pearson-correlation coefficient. Next up, let's turn towards using the cosine-distance as well as z-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Cosine is computed as:\n",
    "\n",
    "$  RawCosine(u,v) = \\frac{\\sum_{k \\in I_{u} \\cap I_{v}} r_{uk}\\cdot r_{uk} }{\\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} r_{uk}^2} \\cdot \\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} r_{vk}^2}} $\n",
    "\n",
    "This considerably simplifies computation. In some implementations, the normalization factors do not constrain on k being in the sets of both users but only one in order to make things feasible:\n",
    "\n",
    "$  FeasibleRawCosine(u,v) = \\frac{\\sum_{k \\in I_{u} \\cap I_{v}} r_{uk}\\cdot r_{uk} }{\\sqrt{\\sum_{k \\in I_{u}} r_{uk}^2} \\cdot \\sqrt{\\sum_{k \\in I_{v}} r_{vk}^2}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RawCosine(table, x: int, y: int):\n",
    "    # x and y are the rows of table on which to perform computation\n",
    "    # get I_x and I_y\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    I_y = np.where(np.isnan(table[y]) == False)[0]\n",
    "    # get the common indices\n",
    "    I = np.intersect1d(I_x, I_y)\n",
    "    # get the numerator\n",
    "    numerator = np.sum(table[x][I]*table[y][I])\n",
    "    # get the denominator\n",
    "    denominator = np.sqrt(np.sum(table[x][I]**2)*np.sum(table[y][I]**2))\n",
    "    # get the correlation\n",
    "    raw_cosine = numerator/denominator\n",
    "    return raw_cosine\n",
    "\n",
    "def FeasibleRawCosine(table, x: int, y: int):\n",
    "    # x and y are the rows of table on which to perform computation\n",
    "    # get I_x and I_y\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    I_y = np.where(np.isnan(table[y]) == False)[0]\n",
    "    # get the common indices\n",
    "    I = np.intersect1d(I_x, I_y)\n",
    "    # get the numerator\n",
    "    numerator = np.sum(table[x][I]*table[y][I])\n",
    "    # get the denominator\n",
    "    denominator = np.sqrt(np.sum(table[x][I_x]**2)*np.sum(table[y][I_y]**2))\n",
    "    # get the correlation\n",
    "    feasible_raw_cosine = numerator/denominator\n",
    "    return feasible_raw_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.956182887467515), np.float64(0.7766217620286883))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RawCosine(table_2_1, 0, 2), FeasibleRawCosine(table_2_1, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we find the Pearson-Correlation to be the preferrable measure as it provides bias-adjustment due to the mean-centering (comp. p. 37).\n",
    "Inference is impacted by the number of common ratings, $ | I_u \\cap I_v | $ between two users u and v. If two users have only few ratings in common, we might want to downweight the impact that this user-pair has on the final imputation. We can do this via significance weighting:\n",
    "\n",
    "$ DiscountedSim(u,v) = Sim(u,v) * \\frac{ \\min\\{|I_u \\cap I_v|, \\beta\\}}{\\beta} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiscountedSim(table, x: int, y: int, beta: float):\n",
    "    # x and y are the rows of table on which to perform computation\n",
    "    # get I_x and I_y\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    I_y = np.where(np.isnan(table[y]) == False)[0]\n",
    "    # get the common indices\n",
    "    I = np.intersect1d(I_x, I_y)\n",
    "    discounted_similarity = pearson_corr(table, x, y) * np.min([1, len(I)/beta])\n",
    "    return discounted_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8944271909999159)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DiscountedSim(table_2_1, 0, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.2 Variants of the Prediction Function\n",
    "\n",
    "We can also use the standard deviation to effectively obtain a (residualized) regression function that uses the z-Scores to predict a user's rating. Effectively, we turn every observed rating into a z-score (demeaned and standardized by SD) of a user, then use for prediction the mean and SD of a user we already know and then use the correlation-function together with the z-score of the same item for other similar users, downweighting by the sum of absolute values of the pearson correlation coefficients:\n",
    "\n",
    "$ \\hat{r}_{uj} = \\mu_u + \\sigma_u \\frac{\\sum_{\\nu \\in P_{u}(j)} Sim(u,v) \\cdot z_{\\nu j}}{\\sum_{\\nu \\in P_{u}(j)} |Sim(u,v)|} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_function(table, x: int):\n",
    "    # x is the row of table on which to perform computation\n",
    "    # get I_x\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    # get the mean of x\n",
    "    x_mean = np.mean(table[x][I_x])\n",
    "    # get the standard deviation\n",
    "    sd = np.sqrt(np.sum((table[x][I_x] - x_mean)**2)/(len(I_x)-1))\n",
    "    return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_vector(table):\n",
    "    # compute the standard deviation for each row\n",
    "    sd_vector = np.zeros(table.shape[0])\n",
    "    for i in range(table.shape[0]):\n",
    "        sd_vector[i] = sd_function(table, i)\n",
    "    return sd_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_table(table):\n",
    "    table_copy = table.copy()\n",
    "    for i in range(table.shape[0]):\n",
    "        I = np.where(np.isnan(table[i]) == False)[0]\n",
    "        table_copy[i][I] = (table[i][I] - np.mean(table[i][I]))/sd_function(table, i)\n",
    "    return table_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_imputation(table, threshold, dimension: int = 0):\n",
    "    # x is the row to be imputed\n",
    "    # y is the list of rows to be used for imputation\n",
    "    # create a normalized copy of the table\n",
    "    table_copy = table.copy()\n",
    "    normalized_table = normalize_table(table)\n",
    "    # compute user sd\n",
    "    sd_vector_store = sd_vector(table)\n",
    "    # compute correlation matrix\n",
    "    corr_matrix = corr_matrix_func(table, dimension)\n",
    "    # compute means\n",
    "    user_mean = mean_compute(table, dimension)\n",
    "    # for each row in correlation matrix compute the k based on threshold\n",
    "    k = [[] for _ in range(corr_matrix.shape[0])]\n",
    "    for i in range(corr_matrix.shape[0]):\n",
    "        k[i] = np.where((corr_matrix[i] > threshold))[0]\n",
    "    # for each value of k, remove the same number if it is in k, e.g. remove 2 from k[2]\n",
    "    for i in range(corr_matrix.shape[0]):\n",
    "        k[i] = np.setdiff1d(k[i], i)\n",
    "    # for each row and column in the table, compute the imputation\n",
    "    for i in range(table.shape[0]):\n",
    "        for j in range(table.shape[1]):\n",
    "            if np.isnan(table[i,j]):\n",
    "                table_copy[i,j] = user_mean[i] + sd_vector_store[i]*np.sum(corr_matrix[i][k[i]]*normalized_table[k[i],j])/np.sum(np.abs(corr_matrix[i][k[i]]))\n",
    "    return table_copy, corr_matrix, user_mean, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_copy, corr_mat, user_means, k_imputed = z_score_imputation(table_2_1, 0.7, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "        4.        ],\n",
       "       [6.        , 7.        , 6.37893144, 4.        , 3.        ,\n",
       "        4.        ],\n",
       "       [3.04146466, 3.        , 3.        , 1.        , 1.        ,\n",
       "        1.10483034],\n",
       "       [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "        4.        ],\n",
       "       [1.        , 1.52326871, 1.        , 2.        , 3.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.4 Impact of the long tail\n",
    "\n",
    "As discussed in the book, similarity weighting can be beneficial if, for example, some items are more often rated than others. For example, we might multiply by the logarithm of the ratio of number of all users to number of users who rated a specific item:\n",
    "\n",
    "$w_k = \\log\\Big(\\frac{m}{m_k}\\Big)$\n",
    "\n",
    "$ WeightedPearson(u,v) = \\frac{\\sum_{k \\in I_{u} \\cap I_{v}} w_k \\cdot (r_{uk}- \\mu_{u}) \\cdot (r_{uk}- \\mu_{u}) }{\\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} w_k \\cdot (r_{uk}- \\mu_{u})^2} \\cdot \\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} w_k \\cdot (r_{vk}- \\mu_{v})^2}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted pearson-correlation-matrix\n",
    "def weighted_pearson(table_input, x: int, y:int, dimension: int = 0):\n",
    "    table = table_input.copy()\n",
    "    if dimension == 1:\n",
    "        table = table.T\n",
    "    # compute the correlation matrix\n",
    "    m = table.shape[0]\n",
    "    weights_pre = [len(np.where(np.isnan(table.T[i]) == False)[0]) for i in range(table.shape[1])]\n",
    "    weights = np.array([np.log(m/weights_pre[i]) for i in range(table.shape[1])])\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    I_y = np.where(np.isnan(table[y]) == False)[0]\n",
    "    # get the common indices\n",
    "    I = np.intersect1d(I_x, I_y)\n",
    "    # get the mean of x and y\n",
    "    x_mean = np.mean(table[x][I])\n",
    "    y_mean = np.mean(table[y][I])\n",
    "    # get the numerator\n",
    "    numerator = np.sum(weights[I]*(table[x][I] - x_mean)*(table[y][I] - y_mean))\n",
    "    # get the denominator\n",
    "    denominator = np.sqrt(np.sum(weights[I] * (table[x][I] - x_mean)**2)*np.sum(weights[I]*(table[y][I] - y_mean)**2))\n",
    "    # get the correlation\n",
    "    corr = numerator/denominator\n",
    "    return corr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.9296696802013684)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_pearson(table_2_1, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_pearson_corr_mat(table, dimension: int = 0):\n",
    "    if dimension == 0:\n",
    "        number = table.shape[dimension]\n",
    "        corr_matrix = np.zeros((number, number))\n",
    "        for i in range(number):\n",
    "            for j in range(number):\n",
    "                corr_matrix[i,j] = weighted_pearson(table, i, j)\n",
    "    elif dimension == 1:\n",
    "        number = table.shape[dimension]\n",
    "        corr_matrix = np.zeros((number, number))\n",
    "        table = table.T\n",
    "        for i in range(number):\n",
    "            for j in range(number):\n",
    "                corr_matrix[i,j] = weighted_pearson(table, i, j)\n",
    "    else:\n",
    "        print(\"Invalid dimension\")\n",
    "        return None\n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.80428683,  0.89442719, -0.92966968, -0.99811498],\n",
       "       [ 0.80428683,  1.        ,  1.        , -0.75028028, -0.92163538],\n",
       "       [ 0.89442719,  1.        ,  1.        , -1.        , -1.        ],\n",
       "       [-0.92966968, -0.75028028, -1.        ,  1.        ,  0.94087507],\n",
       "       [-0.99811498, -0.92163538, -1.        ,  0.94087507,  1.        ]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_pearson_mat = weighted_pearson_corr_mat(table_2_1, 0)\n",
    "weighted_pearson_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Item-Based Neighbourhood Models\n",
    "\n",
    "As described in the book, all of the previously given models were user-centric, i.e. we compared two users in their ratings over all products. For this, we computed the set of relevant products for each user-pair and weighted the rating of another user with an appropriate similarity-measure, e.g. the Pearson-Correlation-Coefficient or Cosine-Similarity.\n",
    "For Item-Based-Methods, we go the other way: we compare items in their similarity and then multiply the ratings of *the same* user for other items multiplied with their similarity for an item at hand. That means if we want to predict how much a user will enjoy a particular brand of car tires, we first compute how similar other items, e.g. windshield wipers, motors, car seats, are to tires, and then predict the user's rating for these car tires as a weighted sum of his ratings for all these other items. \n",
    "In contrast, with user-based methods we weight the ratings of *other users* for the *same item* with the similarity between the two *users*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ AdjustedCosine(i,j) = \\frac{\\sum_{u \\in U_i \\cap U_j} s_{ui} \\cdot s_{uj}}{\\sqrt{\\sum_{u \\in U_i \\cap U_j} s_{ui}^2} \\cdot \\sqrt{\\sum_{u \\in U_i \\cap U_j} s_{ui}^2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $s_{uj}$ is the demeaned rating of user $u$ for item $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 4])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "np.where(np.isnan(table_2_1[:,5]) == False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdjustedCosine(table, i: int, j: int):\n",
    "    # i and j are the columns of table on which to perform computation\n",
    "    # get U_i and U_j\n",
    "    U_i = np.where(np.isnan(table[:,i]) == False)[0]\n",
    "    U_j = np.where(np.isnan(table[:,j]) == False)[0]\n",
    "    # get the common indices\n",
    "    U = np.intersect1d(U_i, U_j)\n",
    "    # get the mean of x and y\n",
    "    i_mean = np.mean(table[U,i])\n",
    "    j_mean = np.mean(table[U,j])\n",
    "    # get the numerator\n",
    "    numerator = np.sum((table[U,i] - i_mean)*(table[U,j] - j_mean))\n",
    "    # get the denominator\n",
    "    denominator = np.sqrt(np.sum((table[U,i] - i_mean)**2)*np.sum((table[U,j] - j_mean)**2))\n",
    "    # get the correlation\n",
    "    corr = numerator/denominator\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8971499589146108)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdjustedCosine(table_2_1, 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having computed the adjusted Cosine, let's use it to predict ratings. In order to do that we need to get the top-k-similar items. We can set k as we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4, 5, 0]), array([0.81348922, 0.87038828, 0.89714996]))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def TopKSimilarities(table, u: int, t: int, k: int = 3):\n",
    "    # compute the top-k adjusted Cosine similarities for user u and item t:\n",
    "    # get the number of items\n",
    "    m = table.shape[1]\n",
    "    # get the similarities\n",
    "    similarities = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        similarities[i] = AdjustedCosine(table, t, i)\n",
    "    # eliminate the similarity of t with itself\n",
    "    similarities[t] = 0\n",
    "    # get the top-k similarities\n",
    "    top_k = np.argsort(similarities)[-k:]\n",
    "    return top_k, similarities[top_k]\n",
    "\n",
    "TopKSimilarities(table_2_1, 0, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdjCosPrediction(table, u: int, t: int, k: int = 3, threshold: float = 0.7):\n",
    "    # compute the prediction for user u and item t using the top-k adjusted Cosine similarities\n",
    "    top_k, similarities = TopKSimilarities(table, u, t, k)\n",
    "    # keep only the similarities above the threshold\n",
    "    top_k = top_k[np.where(similarities > threshold)]\n",
    "    similarities = similarities[np.where(similarities > threshold)]\n",
    "    # get the mean of the user\n",
    "    user_mean = np.mean(table[u][np.where(np.isnan(table[u]) == False)[0]])\n",
    "    # get the prediction\n",
    "    prediction = user_mean + np.sum(similarities*(table[u][top_k] - np.mean(table[u][np.where(np.isnan(table[u]) == False)[0]])))/np.sum(np.abs(similarities))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.357962731499528)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdjCosPrediction(table_2_1, 0, 3, 3, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imputation function that takes as input a table and returns the imputed table where the imputation is done using the top-k adjusted cosine similarity if an entry is missing\n",
    "def AdjCosImputation(table, k: int = 3, threshold: float = 0.7):\n",
    "    # create a copy of the table\n",
    "    table_copy = table.copy()\n",
    "    # for each row and column in the table, compute the imputation\n",
    "    for i in range(table.shape[0]):\n",
    "        for j in range(table.shape[1]):\n",
    "            if np.isnan(table[i,j]):\n",
    "                table_copy[i,j] = AdjCosPrediction(table, i, j, k, threshold)\n",
    "    return table_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us replicate the computation on p. 41:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maggo\\AppData\\Local\\Temp\\ipykernel_4888\\229472857.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  corr = numerator/denominator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "        4.        ],\n",
       "       [6.        , 7.        , 6.50271747, 4.        , 3.        ,\n",
       "        4.        ],\n",
       "       [3.        , 3.        , 3.        , 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "        4.        ],\n",
       "       [1.        , 1.        , 1.        , 2.        , 3.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1_adj_cosine_imputed_items = AdjCosImputation(table_2_1, 2, 0.7)\n",
    "table_2_1_adj_cosine_imputed_items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
