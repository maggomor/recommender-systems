{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import standard packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set-Up of Matrices for Collaborative filtering\n",
    "\n",
    "First we want to implement the ratings matrix from Table 2.1 featuring 5 users and 6 items. We want to employ a simple user-based and item-based model for imputing ratings. For that we will use the cosine and the pearson-correlation-coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables 2.1 and 2.2\n",
    "\n",
    "table_2_1 = np.array([[7, 6, 7, 4, 5 , 4], [6,7,np.nan, 4,3,4], [np.nan, 3, 3, 1, 1, np.nan], [1, 2, 2 , 3 ,3 , 4], [1, np.nan, 1, 2, 3, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to compute the pearson-correlation-coefficient. The formula for this is:\n",
    "\n",
    "$ \\mu_u = \\frac{\\sum_{k \\in I_{u}} r_{uk}}{|I_{u}|} $\n",
    "\n",
    "$  Sim(u,v) = Pearson(u,v) = \\frac{\\sum_{k \\in I_{u} \\cap I_{v}} (r_{uk}- \\mu_{u}) \\cdot (r_{uk}- \\mu_{u}) }{\\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} (r_{uk}- \\mu_{u})^2} \\cdot \\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} (r_{vk}- \\mu_{v})^2}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(table, x: int, y: int):\n",
    "    # x and y are the rows of table on which to perform computation\n",
    "    # get I_x and I_y\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    I_y = np.where(np.isnan(table[y]) == False)[0]\n",
    "    # get the common indices\n",
    "    I = np.intersect1d(I_x, I_y)\n",
    "    # get the mean of x and y\n",
    "    x_mean = np.mean(table[x][I])\n",
    "    y_mean = np.mean(table[y][I])\n",
    "    # get the numerator\n",
    "    numerator = np.sum((table[x][I] - x_mean)*(table[y][I] - y_mean))\n",
    "    # get the denominator\n",
    "    denominator = np.sqrt(np.sum((table[x][I] - x_mean)**2)*np.sum((table[y][I] - y_mean)**2))\n",
    "    # get the correlation\n",
    "    corr = numerator/denominator\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the correlation coefficient for rows 0 and 2 (1 and 3 in the book). In the book, it is given as 0.894."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8944271909999159)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_corr(table_2_1, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, we can also compute the entire correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "       [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "       [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "       [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "       [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a correlation matrix for the table\n",
    "corr_matrix = np.zeros((5,5))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        corr_matrix[i,j] = pearson_corr(table_2_1, i, j)\n",
    "\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix_func(table, dimension):\n",
    "    if dimension == 0:\n",
    "        number = table.shape[dimension]\n",
    "        corr_matrix = np.zeros((number, number))\n",
    "        for i in range(number):\n",
    "            for j in range(number):\n",
    "                corr_matrix[i,j] = pearson_corr(table, i, j)\n",
    "    elif dimension == 1:\n",
    "        number = table.shape[dimension]\n",
    "        corr_matrix = np.zeros((number, number))\n",
    "        table = table.T\n",
    "        for i in range(number):\n",
    "            for j in range(number):\n",
    "                corr_matrix[i,j] = pearson_corr(table, i, j)\n",
    "    else:\n",
    "        print(\"Invalid dimension\")\n",
    "        return None\n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "       [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "       [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "       [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "       [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix_func(table = table_2_1, dimension = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maggo\\AppData\\Local\\Temp\\ipykernel_12308\\2376229201.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  corr = numerator/denominator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "        [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "        [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "        [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "        [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]]),\n",
       " array([[1.        , 0.94063416, 0.98782916, 0.89714996, 0.67675297,\n",
       "         0.57263713],\n",
       "        [0.94063416, 1.        , 0.99862543, 0.69310328, 0.51449576,\n",
       "                nan],\n",
       "        [0.98782916, 0.99862543, 1.        , 0.6381449 , 0.62092042,\n",
       "         0.62861856],\n",
       "        [0.89714996, 0.69310328, 0.6381449 , 1.        , 0.81348922,\n",
       "         0.87038828],\n",
       "        [0.67675297, 0.51449576, 0.62092042, 0.81348922, 1.        ,\n",
       "         0.33333333],\n",
       "        [0.57263713,        nan, 0.62861856, 0.87038828, 0.33333333,\n",
       "         1.        ]]))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix_func(table_2_1, 0), corr_matrix_func(table_2_1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we can compute the correlation coefficient both for rows or columns.\n",
    "\n",
    "Next, we can demean the rating in every line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demean_func(table):\n",
    "    # demean the table\n",
    "    new_table = np.zeros(table.shape)\n",
    "    for i in range(table.shape[0]):\n",
    "        I = np.where(np.isnan(table[i]) == False)[0]\n",
    "        new_table[i][I] = table[i][I] - np.mean(table[i][I])\n",
    "    return new_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5,  0.5,  1.5, -1.5, -0.5, -1.5],\n",
       "       [ 1.2,  2.2,  0. , -0.8, -1.8, -0.8],\n",
       "       [ 0. ,  1. ,  1. , -1. , -1. ,  0. ],\n",
       "       [-1.5, -0.5, -0.5,  0.5,  0.5,  1.5],\n",
       "       [-1. ,  0. , -1. ,  0. ,  1. ,  1. ]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_2 = demean_func(table_2_1)\n",
    "table_2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to store the means somewhere. Let us write a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.5, 4.8, 2. , 2.5, 2. ])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_compute(table, dimension = 0):\n",
    "    # compute the mean of the table\n",
    "    if dimension == 1:\n",
    "        table_copy = table.T\n",
    "    else:\n",
    "        table_copy = table\n",
    "    mean = np.zeros(table_copy.shape[0])\n",
    "    for i in range(table_copy.shape[0]):\n",
    "        I = np.where(np.isnan(table_copy[i]) == False)[0]\n",
    "        mean[i] = np.mean(table_copy[i][I])\n",
    "    return mean\n",
    "\n",
    "user_mean = mean_compute(table_2_1, 0)\n",
    "user_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.75, 4.5 , 3.25, 2.8 , 3.  , 3.75])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_compute(table_2_1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the demeaned table to better predict a rating. Suppose that we want to predict for user in 3 (row-index 2) their ratings for movies 1 and 6 (indices 0 and 5, respectively). We can first compute the set of relevant neighbours as those whose correlation exceeds 0.7 (as an arbitrary benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "       [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "       [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "       [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "       [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix_table_2_2 = corr_matrix_func(table_2_1, 0)\n",
    "corr_matrix_table_2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from themselves, this is the case for users 1 and 2 (indices 0 and 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = np.where((corr_matrix_table_2_2[2] > 0.7))\n",
    "k = k[0][:-1]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "        4.        ],\n",
       "       [6.        , 7.        ,        nan, 4.        , 3.        ,\n",
       "        4.        ],\n",
       "       [3.34386392, 3.        , 3.        , 1.        , 1.        ,\n",
       "               nan],\n",
       "       [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "        4.        ],\n",
       "       [1.        ,        nan, 1.        , 2.        , 3.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a deep copy of table_2_2\n",
    "table_2_1_copy = table_2_1.copy()\n",
    "table_2_1_copy[2,0] = user_mean[2] + np.sum(corr_matrix_table_2_2[2][k]*table_2_2[k,0])/np.sum(np.abs(corr_matrix_table_2_2[2][k]))\n",
    "table_2_1_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us encapsulate this in a function. We will have to specify a correlation-threshold to determine k, and then within the function we create a deep-copy that fills in the NaN-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_function(table, threshold, dimension):\n",
    "    table_copy = table.copy()\n",
    "    # compute correlation_matrix\n",
    "    corr_matrix = corr_matrix_func(table, dimension)\n",
    "    # compute means\n",
    "    user_mean = mean_compute(table, dimension)\n",
    "    # demean the table\n",
    "    table_copy_dm = demean_func(table_copy)\n",
    "    # for each row in correlation matrix compute the k based on threshold\n",
    "    k = [[] for _ in range(corr_matrix.shape[0])]\n",
    "    for i in range(corr_matrix.shape[0]):\n",
    "        k[i] = np.where((corr_matrix[i] > threshold))[0]\n",
    "    # for each value of k, remove the same number if it is in k, e.g. remove 2 from k[2]\n",
    "    for i in range(corr_matrix.shape[0]):\n",
    "        k[i] = np.setdiff1d(k[i], i)\n",
    "    # for each row and column in the table, compute the imputation\n",
    "    for i in range(table.shape[0]):\n",
    "        for j in range(table.shape[1]):\n",
    "            if np.isnan(table[i,j]):\n",
    "                table_copy[i,j] = user_mean[i] + np.sum(corr_matrix[i][k[i]]*table_copy_dm[k[i],j])/np.sum(np.abs(corr_matrix[i][k[i]]))\n",
    "    return table_copy, corr_matrix, user_mean, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2_1_imputed, corr_matrix_comp, user_mean_comp, k_comp = imputation_function(table_2_1, 0.7, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "        4.        ],\n",
       "       [6.        , 7.        , 6.0135157 , 4.        , 3.        ,\n",
       "        4.        ],\n",
       "       [3.34386392, 3.        , 3.        , 1.        , 1.        ,\n",
       "        0.86431752],\n",
       "       [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "        4.        ],\n",
       "       [1.        , 1.5       , 1.        , 2.        , 3.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "       [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "       [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "       [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "       [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.5, 4.8, 2. , 2.5, 2. ])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_mean_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2]), array([0, 2]), array([0, 1]), array([4]), array([3])]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interestingly enough, we can do the entire thing now as well for items!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maggo\\AppData\\Local\\Temp\\ipykernel_12308\\2376229201.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  corr = numerator/denominator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "        4.        ],\n",
       "       [6.        , 7.        , 5.74252405, 4.        , 3.        ,\n",
       "        4.        ],\n",
       "       [4.59918476, 3.        , 3.        , 1.        , 1.        ,\n",
       "        2.10190223],\n",
       "       [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "        4.        ],\n",
       "       [1.        , 2.5       , 1.        , 2.        , 3.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1_imputed_cols, corr_matrix_comp_cols, user_mean_comp_cols, k_comp_cols = imputation_function(table_2_1, 0.7, 1)\n",
    "table_2_1_imputed_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.94063416, 0.98782916, 0.89714996, 0.67675297,\n",
       "        0.57263713],\n",
       "       [0.94063416, 1.        , 0.99862543, 0.69310328, 0.51449576,\n",
       "               nan],\n",
       "       [0.98782916, 0.99862543, 1.        , 0.6381449 , 0.62092042,\n",
       "        0.62861856],\n",
       "       [0.89714996, 0.69310328, 0.6381449 , 1.        , 0.81348922,\n",
       "        0.87038828],\n",
       "       [0.67675297, 0.51449576, 0.62092042, 0.81348922, 1.        ,\n",
       "        0.33333333],\n",
       "       [0.57263713,        nan, 0.62861856, 0.87038828, 0.33333333,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix_comp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3]),\n",
       " array([0, 2]),\n",
       " array([0, 1]),\n",
       " array([0, 4, 5]),\n",
       " array([3]),\n",
       " array([3])]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_comp_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.1 Similarity Function Variants\n",
    "\n",
    "We have now computed using the pearson-correlation coefficient. Next up, let's turn towards using the cosine-distance as well as z-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Cosine is computed as:\n",
    "\n",
    "$  RawCosine(u,v) = \\frac{\\sum_{k \\in I_{u} \\cap I_{v}} r_{uk}\\cdot r_{uk} }{\\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} r_{uk}^2} \\cdot \\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} r_{vk}^2}} $\n",
    "\n",
    "This considerably simplifies computation. In some implementations, the normalization factors do not constrain on k being in the sets of both users but only one in order to make things feasible:\n",
    "\n",
    "$  FeasibleRawCosine(u,v) = \\frac{\\sum_{k \\in I_{u} \\cap I_{v}} r_{uk}\\cdot r_{uk} }{\\sqrt{\\sum_{k \\in I_{u}} r_{uk}^2} \\cdot \\sqrt{\\sum_{k \\in I_{v}} r_{vk}^2}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RawCosine(table, x: int, y: int):\n",
    "    # x and y are the rows of table on which to perform computation\n",
    "    # get I_x and I_y\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    I_y = np.where(np.isnan(table[y]) == False)[0]\n",
    "    # get the common indices\n",
    "    I = np.intersect1d(I_x, I_y)\n",
    "    # get the numerator\n",
    "    numerator = np.sum(table[x][I]*table[y][I])\n",
    "    # get the denominator\n",
    "    denominator = np.sqrt(np.sum(table[x][I]**2)*np.sum(table[y][I]**2))\n",
    "    # get the correlation\n",
    "    raw_cosine = numerator/denominator\n",
    "    return raw_cosine\n",
    "\n",
    "def FeasibleRawCosine(table, x: int, y: int):\n",
    "    # x and y are the rows of table on which to perform computation\n",
    "    # get I_x and I_y\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    I_y = np.where(np.isnan(table[y]) == False)[0]\n",
    "    # get the common indices\n",
    "    I = np.intersect1d(I_x, I_y)\n",
    "    # get the numerator\n",
    "    numerator = np.sum(table[x][I]*table[y][I])\n",
    "    # get the denominator\n",
    "    denominator = np.sqrt(np.sum(table[x][I_x]**2)*np.sum(table[y][I_y]**2))\n",
    "    # get the correlation\n",
    "    feasible_raw_cosine = numerator/denominator\n",
    "    return feasible_raw_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.956182887467515), np.float64(0.7766217620286883))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RawCosine(table_2_1, 0, 2), FeasibleRawCosine(table_2_1, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we find the Pearson-Correlation to be the preferrable measure as it provides bias-adjustment due to the mean-centering (comp. p. 37).\n",
    "Inference is impacted by the number of common ratings, $ | I_u \\cap I_v | $ between two users u and v. If two users have only few ratings in common, we might want to downweight the impact that this user-pair has on the final imputation. We can do this via significance weighting:\n",
    "\n",
    "$ DiscountedSim(u,v) = Sim(u,v) * \\frac{ \\min\\{|I_u \\cap I_v|, \\beta\\}}{\\beta} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiscountedSim(table, x: int, y: int, beta: float):\n",
    "    # x and y are the rows of table on which to perform computation\n",
    "    # get I_x and I_y\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    I_y = np.where(np.isnan(table[y]) == False)[0]\n",
    "    # get the common indices\n",
    "    I = np.intersect1d(I_x, I_y)\n",
    "    discounted_similarity = pearson_corr(table, x, y) * np.min([1, len(I)/beta])\n",
    "    return discounted_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8944271909999159)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DiscountedSim(table_2_1, 0, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.2 Variants of the Prediction Function\n",
    "\n",
    "We can also use the standard deviation to effectively obtain a (residualized) regression function that uses the z-Scores to predict a user's rating. Effectively, we turn every observed rating into a z-score (demeaned and standardized by SD) of a user, then use for prediction the mean and SD of a user we already know and then use the correlation-function together with the z-score of the same item for other similar users, downweighting by the sum of absolute values of the pearson correlation coefficients:\n",
    "\n",
    "$ \\hat{r}_{uj} = \\mu_u + \\sigma_u \\frac{\\sum_{\\nu \\in P_{u}(j)} Sim(u,v) \\cdot z_{\\nu j}}{\\sum_{\\nu \\in P_{u}(j)} |Sim(u,v)|} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_function(table, x: int):\n",
    "    # x is the row of table on which to perform computation\n",
    "    # get I_x\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    # get the mean of x\n",
    "    x_mean = np.mean(table[x][I_x])\n",
    "    # get the standard deviation\n",
    "    sd = np.sqrt(np.sum((table[x][I_x] - x_mean)**2)/(len(I_x)-1))\n",
    "    return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_vector(table):\n",
    "    # compute the standard deviation for each row\n",
    "    sd_vector = np.zeros(table.shape[0])\n",
    "    for i in range(table.shape[0]):\n",
    "        sd_vector[i] = sd_function(table, i)\n",
    "    return sd_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_table(table):\n",
    "    table_copy = table.copy()\n",
    "    for i in range(table.shape[0]):\n",
    "        I = np.where(np.isnan(table[i]) == False)[0]\n",
    "        table_copy[i][I] = (table[i][I] - np.mean(table[i][I]))/sd_function(table, i)\n",
    "    return table_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_imputation(table, threshold, dimension: int = 0):\n",
    "    # x is the row to be imputed\n",
    "    # y is the list of rows to be used for imputation\n",
    "    # create a normalized copy of the table\n",
    "    table_copy = table.copy()\n",
    "    normalized_table = normalize_table(table)\n",
    "    # compute user sd\n",
    "    sd_vector_store = sd_vector(table)\n",
    "    # compute correlation matrix\n",
    "    corr_matrix = corr_matrix_func(table, dimension)\n",
    "    # compute means\n",
    "    user_mean = mean_compute(table, dimension)\n",
    "    # for each row in correlation matrix compute the k based on threshold\n",
    "    k = [[] for _ in range(corr_matrix.shape[0])]\n",
    "    for i in range(corr_matrix.shape[0]):\n",
    "        k[i] = np.where((corr_matrix[i] > threshold))[0]\n",
    "    # for each value of k, remove the same number if it is in k, e.g. remove 2 from k[2]\n",
    "    for i in range(corr_matrix.shape[0]):\n",
    "        k[i] = np.setdiff1d(k[i], i)\n",
    "    # for each row and column in the table, compute the imputation\n",
    "    for i in range(table.shape[0]):\n",
    "        for j in range(table.shape[1]):\n",
    "            if np.isnan(table[i,j]):\n",
    "                table_copy[i,j] = user_mean[i] + sd_vector_store[i]*np.sum(corr_matrix[i][k[i]]*normalized_table[k[i],j])/np.sum(np.abs(corr_matrix[i][k[i]]))\n",
    "    return table_copy, corr_matrix, user_mean, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_copy, corr_mat, user_means, k_imputed = z_score_imputation(table_2_1, 0.7, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "        4.        ],\n",
       "       [6.        , 7.        , 6.37893144, 4.        , 3.        ,\n",
       "        4.        ],\n",
       "       [3.04146466, 3.        , 3.        , 1.        , 1.        ,\n",
       "        1.10483034],\n",
       "       [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "        4.        ],\n",
       "       [1.        , 1.52326871, 1.        , 2.        , 3.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.4 Impact of the long tail\n",
    "\n",
    "As discussed in the book, similarity weighting can be beneficial if, for example, some items are more often rated than others. For example, we might multiply by the logarithm of the ratio of number of all users to number of users who rated a specific item:\n",
    "\n",
    "$w_k = \\log\\Big(\\frac{m}{m_k}\\Big)$\n",
    "\n",
    "$ WeightedPearson(u,v) = \\frac{\\sum_{k \\in I_{u} \\cap I_{v}} w_k \\cdot (r_{uk}- \\mu_{u}) \\cdot (r_{uk}- \\mu_{u}) }{\\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} w_k \\cdot (r_{uk}- \\mu_{u})^2} \\cdot \\sqrt{\\sum_{k \\in I_{u}\\cap I_{v}} w_k \\cdot (r_{vk}- \\mu_{v})^2}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted pearson-correlation-matrix\n",
    "def weighted_pearson(table_input, x: int, y:int, dimension: int = 0):\n",
    "    table = table_input.copy()\n",
    "    if dimension == 1:\n",
    "        table = table.T\n",
    "    # compute the correlation matrix\n",
    "    m = table.shape[0]\n",
    "    weights_pre = [len(np.where(np.isnan(table.T[i]) == False)[0]) for i in range(table.shape[1])]\n",
    "    weights = np.array([np.log(m/weights_pre[i]) for i in range(table.shape[1])])\n",
    "    I_x = np.where(np.isnan(table[x]) == False)[0]\n",
    "    I_y = np.where(np.isnan(table[y]) == False)[0]\n",
    "    # get the common indices\n",
    "    I = np.intersect1d(I_x, I_y)\n",
    "    # get the mean of x and y\n",
    "    x_mean = np.mean(table[x][I])\n",
    "    y_mean = np.mean(table[y][I])\n",
    "    # get the numerator\n",
    "    numerator = np.sum(weights[I]*(table[x][I] - x_mean)*(table[y][I] - y_mean))\n",
    "    # get the denominator\n",
    "    denominator = np.sqrt(np.sum(weights[I] * (table[x][I] - x_mean)**2)*np.sum(weights[I]*(table[y][I] - y_mean)**2))\n",
    "    # get the correlation\n",
    "    corr = numerator/denominator\n",
    "    return corr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.9296696802013684)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_pearson(table_2_1, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_pearson_corr_mat(table, dimension: int = 0):\n",
    "    if dimension == 0:\n",
    "        number = table.shape[dimension]\n",
    "        corr_matrix = np.zeros((number, number))\n",
    "        for i in range(number):\n",
    "            for j in range(number):\n",
    "                corr_matrix[i,j] = weighted_pearson(table, i, j)\n",
    "    elif dimension == 1:\n",
    "        number = table.shape[dimension]\n",
    "        corr_matrix = np.zeros((number, number))\n",
    "        table = table.T\n",
    "        for i in range(number):\n",
    "            for j in range(number):\n",
    "                corr_matrix[i,j] = weighted_pearson(table, i, j)\n",
    "    else:\n",
    "        print(\"Invalid dimension\")\n",
    "        return None\n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.80428683,  0.89442719, -0.92966968, -0.99811498],\n",
       "       [ 0.80428683,  1.        ,  1.        , -0.75028028, -0.92163538],\n",
       "       [ 0.89442719,  1.        ,  1.        , -1.        , -1.        ],\n",
       "       [-0.92966968, -0.75028028, -1.        ,  1.        ,  0.94087507],\n",
       "       [-0.99811498, -0.92163538, -1.        ,  0.94087507,  1.        ]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_pearson_mat = weighted_pearson_corr_mat(table_2_1, 0)\n",
    "weighted_pearson_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Item-Based Neighbourhood Models\n",
    "\n",
    "As described in the book, all of the previously given models were user-centric, i.e. we compared two users in their ratings over all products. For this, we computed the set of relevant products for each user-pair and weighted the rating of another user with an appropriate similarity-measure, e.g. the Pearson-Correlation-Coefficient or Cosine-Similarity.\n",
    "For Item-Based-Methods, we go the other way: we compare items in their similarity and then multiply the ratings of *the same* user for other items multiplied with their similarity for an item at hand. That means if we want to predict how much a user will enjoy a particular brand of car tires, we first compute how similar other items, e.g. windshield wipers, motors, car seats, are to tires, and then predict the user's rating for these car tires as a weighted sum of his ratings for all these other items. \n",
    "In contrast, with user-based methods we weight the ratings of *other users* for the *same item* with the similarity between the two *users*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ AdjustedCosine(i,j) = \\frac{\\sum_{u \\in U_i \\cap U_j} s_{ui} \\cdot s_{uj}}{\\sqrt{\\sum_{u \\in U_i \\cap U_j} s_{ui}^2} \\cdot \\sqrt{\\sum_{u \\in U_i \\cap U_j} s_{ui}^2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $s_{uj}$ is the demeaned rating of user $u$ for item $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 4])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "np.where(np.isnan(table_2_1[:,5]) == False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdjustedCosine(table, i: int, j: int):\n",
    "    # i and j are the columns of table on which to perform computation\n",
    "    # get U_i and U_j\n",
    "    U_i = np.where(np.isnan(table[:,i]) == False)[0]\n",
    "    U_j = np.where(np.isnan(table[:,j]) == False)[0]\n",
    "    # get the common indices\n",
    "    U = np.intersect1d(U_i, U_j)\n",
    "    # get the mean of x and y\n",
    "    i_mean = np.mean(table[U,i])\n",
    "    j_mean = np.mean(table[U,j])\n",
    "    # get the numerator\n",
    "    numerator = np.sum((table[U,i] - i_mean)*(table[U,j] - j_mean))\n",
    "    # get the denominator\n",
    "    denominator = np.sqrt(np.sum((table[U,i] - i_mean)**2)*np.sum((table[U,j] - j_mean)**2))\n",
    "    # get the correlation\n",
    "    corr = numerator/denominator\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8971499589146108)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdjustedCosine(table_2_1, 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having computed the adjusted Cosine, let's use it to predict ratings. In order to do that we need to get the top-k-similar items. We can set k as we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4, 5, 0]), array([0.81348922, 0.87038828, 0.89714996]))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def TopKSimilarities(table, u: int, t: int, k: int = 3):\n",
    "    # compute the top-k adjusted Cosine similarities for user u and item t:\n",
    "    # get the number of items\n",
    "    m = table.shape[1]\n",
    "    # get the similarities\n",
    "    similarities = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        similarities[i] = AdjustedCosine(table, t, i)\n",
    "    # eliminate the similarity of t with itself\n",
    "    similarities[t] = 0\n",
    "    # get the top-k similarities\n",
    "    top_k = np.argsort(similarities)[-k:]\n",
    "    return top_k, similarities[top_k]\n",
    "\n",
    "TopKSimilarities(table_2_1, 0, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdjCosPrediction(table, u: int, t: int, k: int = 3, threshold: float = 0.7):\n",
    "    # compute the prediction for user u and item t using the top-k adjusted Cosine similarities\n",
    "    top_k, similarities = TopKSimilarities(table, u, t, k)\n",
    "    # keep only the similarities above the threshold\n",
    "    top_k = top_k[np.where(similarities > threshold)]\n",
    "    similarities = similarities[np.where(similarities > threshold)]\n",
    "    # get the mean of the user\n",
    "    user_mean = np.mean(table[u][np.where(np.isnan(table[u]) == False)[0]])\n",
    "    # get the prediction\n",
    "    prediction = user_mean + np.sum(similarities*(table[u][top_k] - np.mean(table[u][np.where(np.isnan(table[u]) == False)[0]])))/np.sum(np.abs(similarities))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.357962731499528)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdjCosPrediction(table_2_1, 0, 3, 3, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imputation function that takes as input a table and returns the imputed table where the imputation is done using the top-k adjusted cosine similarity if an entry is missing\n",
    "def AdjCosImputation(table, k: int = 3, threshold: float = 0.7):\n",
    "    # create a copy of the table\n",
    "    table_copy = table.copy()\n",
    "    # for each row and column in the table, compute the imputation\n",
    "    for i in range(table.shape[0]):\n",
    "        for j in range(table.shape[1]):\n",
    "            if np.isnan(table[i,j]):\n",
    "                table_copy[i,j] = AdjCosPrediction(table, i, j, k, threshold)\n",
    "    return table_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us replicate the computation on p. 41:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maggo\\AppData\\Local\\Temp\\ipykernel_12308\\229472857.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  corr = numerator/denominator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "        4.        ],\n",
       "       [6.        , 7.        , 6.50271747, 4.        , 3.        ,\n",
       "        4.        ],\n",
       "       [3.        , 3.        , 3.        , 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "        4.        ],\n",
       "       [1.        , 1.        , 1.        , 2.        , 3.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1_adj_cosine_imputed_items = AdjCosImputation(table_2_1, 2, 0.7)\n",
    "table_2_1_adj_cosine_imputed_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.6 A unified view of user-based and item-based methods\n",
    "\n",
    "As described on p.45, one can also combine user-based-methods with item-based methods. In the following, I will present a method to impute missing values using pearson-correlation-coefficient between rows and between columns separately and weighting the imputations with the pearson-correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneralizedImputation(table, threshold):\n",
    "    table_copy = table.copy()\n",
    "    # compute correlation_matrix\n",
    "    user_corr_matrix = corr_matrix_func(table_copy, dimension=0)\n",
    "    item_corr_matrix = corr_matrix_func(table_copy, dimension=1)\n",
    "    # compute means\n",
    "    user_mean = mean_compute(table_copy, dimension=0)\n",
    "    sd_vector_store = sd_vector(table_copy)\n",
    "    # demean the table\n",
    "    table_copy_dm = normalize_table(table_copy)\n",
    "    # for each row in correlation matrix compute the k based on threshold\n",
    "    k = [[] for _ in range(user_corr_matrix.shape[0])]\n",
    "    for i in range(user_corr_matrix.shape[0]):\n",
    "        k[i] = np.where((user_corr_matrix[i] > threshold))[0]\n",
    "    # for each value of k, remove the same number if it is in k, e.g. remove 2 from k[2]\n",
    "    for i in range(user_corr_matrix.shape[0]):\n",
    "        k[i] = np.setdiff1d(k[i], i)\n",
    "    # for each column in correlation matrix compute the k based on threshold\n",
    "    k_cols = [[] for _ in range(item_corr_matrix.shape[1])]\n",
    "    for i in range(item_corr_matrix.shape[1]):\n",
    "        k_cols[i] = np.where((item_corr_matrix[:,i] > threshold))[0]\n",
    "    # for each value of k, remove the same number if it is in k, e.g. remove 2 from k[2]\n",
    "    for i in range(item_corr_matrix.shape[1]):\n",
    "        k_cols[i] = np.setdiff1d(k_cols[i], i)\n",
    "    # for each row and column in the table, perform imputation as follows\n",
    "    # r_ij = user_mean[i] + sd_vector_store[i]*weighted_rating if table[i,j] is missing\n",
    "    # weighted_rating = sum(user_corr_matrix[i][k[i]]*table_copy_dm[k[i],j])/sum(np.abs(user_corr_matrix[i][k[i]])) * (sum(np.abs(user_corr_matrix[i][k[i]])) / (sum(np.abs(user_corr_matrix[i][k[i]])) + sum(np.abs(item_corr_matrix[j][k_cols[j]]))) + sum(item_corr_matrix[j][k_cols[j]]*table_copy_dm[i,k_cols[j]])/sum(np.abs(item_corr_matrix[j][k_cols[j]]))\n",
    "    for i in range(table.shape[0]):\n",
    "        for j in range(table.shape[1]):\n",
    "            if np.isnan(table[i,j]):\n",
    "                weighted_rating = np.sum(user_corr_matrix[i][k[i]]*table_copy_dm[k[i],j])/np.sum(np.abs(user_corr_matrix[i][k[i]])) * (np.sum(np.abs(user_corr_matrix[i][k[i]])) / (np.sum(np.abs(user_corr_matrix[i][k[i]])) + np.sum(np.abs(item_corr_matrix[j][k_cols[j]])))) + np.sum(item_corr_matrix[j][k_cols[j]]*table_copy_dm[i,k_cols[j]])/np.sum(np.abs(item_corr_matrix[j][k_cols[j]])) * (np.sum(np.abs(item_corr_matrix[j][k_cols[j]])) / (np.sum(np.abs(user_corr_matrix[i][k[i]])) + np.sum(np.abs(item_corr_matrix[j][k_cols[j]]))))\n",
    "                table_copy[i,j] = user_mean[i] + sd_vector_store[i]*weighted_rating\n",
    "    return table_copy, corr_matrix, user_mean, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maggo\\AppData\\Local\\Temp\\ipykernel_12308\\2376229201.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  corr = numerator/denominator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "        4.        ],\n",
       "       [6.        , 7.        , 6.44573888, 4.        , 3.        ,\n",
       "        4.        ],\n",
       "       [2.63396979, 3.        , 3.        , 1.        , 1.        ,\n",
       "        1.07147565],\n",
       "       [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "        4.        ],\n",
       "       [1.        , 1.16295641, 1.        , 2.        , 3.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1_gen_imputed, corr_matrix_gen, user_mean_gen, k_gen = GeneralizedImputation(table_2_1, 0.7)\n",
    "table_2_1_gen_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have imputed the rating in the following fashion:\n",
    "\n",
    "$ \\hat{r}_{ij} = \\mu_i + \\sigma_i \\cdot \\Big[ \\frac{\\sum_{\\nu \\in P_u(j)} Sim(u, \\nu) \\cdot z_{\\nu j}}{\\sum_{\\nu \\in P_u(j)} |Sim(u,v)|}\\cdot\\frac{\\sum_{\\nu \\in P_u(j)} |Sim(u,v)|}{\\sum_{\\nu \\in P_u(j)} |Sim(u,v)| + \\sum_{t \\in Q_j(i)} |ItemSim(j, t)|} + \n",
    "\\frac{\\sum_{t \\in Q_j(i)} ItemSim(j, t) \\cdot z_{i,t}}{\\sum_{t \\in Q_j(i)} |ItemSim(j, t)|} \\cdot\\frac{\\sum_{t \\in Q_j(i)} |ItemSim(j, t)|}{\\sum_{\\nu \\in P_u(j)} |Sim(u,v)| + \\sum_{t \\in Q_j(i)} |ItemSim(j, t)|} \\Big]$\n",
    "\n",
    "where\n",
    "\n",
    "$ \\sigma_u = \\sqrt{\\frac{\\sum_{j \\in I_u} (r_{uj} - \\mu_u)^2}{|I_u| - 1}} $ is the standard deviation for user *u*,\n",
    "\n",
    "$ z_{uj} = \\frac{r_{uj} - \\mu_u}{\\sigma_u} = \\frac{s_{uj}}{\\sigma_u}$ is the z-score of item *j* for user *u*, and \n",
    "\n",
    "$ Sim(u,v) $ is the Pearson-Correlation on user-level and $ ItemSim(j,t) $ is the Pearson-Correlation on item-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "This imputation procedure is not given verbatim in Aggarwal (2016) but is my interpretation of the description on p. 45. He effectively describes taking a weighted average of the prediction from the user-based-method and the item-based-method. The most straightforward weights that fulfill the requirements in \"2.\" are the ones that I have used above: linear weights. However, nothing prevents the interested reader from implementing a different weighting scheme; for example one that is non-linear. A harmonic or geometric mean are just as possible as the arithmetic mean that I have given above. Similarly, one can also implement this method using as a similarity-measure the Adjusted Cosine. The way to implement that would be to create an AdjustedCosine-Similarity-Matrix just as one constructs the correlation-matrix, once on user-level, once on item-level, and then use those similarities. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdjustedCosineComputation(table, i: int, j: int, dimension: int = 1):\n",
    "    # i and j are the columns of table on which to perform computation\n",
    "    # get U_i and U_j\n",
    "    table_copy = table.copy()\n",
    "    if dimension == 0:\n",
    "        table_copy = table_copy.T\n",
    "    U_i = np.where(np.isnan(table_copy[:,i]) == False)[0]\n",
    "    U_j = np.where(np.isnan(table_copy[:,j]) == False)[0]\n",
    "    # get the common indices\n",
    "    U = np.intersect1d(U_i, U_j)\n",
    "    # get the mean of x and y\n",
    "    i_mean = np.mean(table_copy[U,i])\n",
    "    j_mean = np.mean(table_copy[U,j])\n",
    "    # get the numerator\n",
    "    numerator = np.sum((table_copy[U,i] - i_mean)*(table_copy[U,j] - j_mean))\n",
    "    # get the denominator\n",
    "    denominator = np.sqrt(np.sum((table_copy[U,i] - i_mean)**2)*np.sum((table_copy[U,j] - j_mean)**2))\n",
    "    # get the correlation\n",
    "    corr = numerator/denominator\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdjustedCosineSimilarityMatrix(table, dimension: int = 0):\n",
    "    # compute the adjusted cosine similarity matrix\n",
    "    m = table.shape[dimension]\n",
    "    adj_cos_sim_matrix = np.zeros((m,m))\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            adj_cos_sim_matrix[i,j] = AdjustedCosineComputation(table, i, j, dimension = dimension)\n",
    "    return adj_cos_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maggo\\AppData\\Local\\Temp\\ipykernel_12308\\3055660411.py:19: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  corr = numerator/denominator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "        [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "        [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "        [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "        [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]]),\n",
       " array([[1.        , 0.94063416, 0.98782916, 0.89714996, 0.67675297,\n",
       "         0.57263713],\n",
       "        [0.94063416, 1.        , 0.99862543, 0.69310328, 0.51449576,\n",
       "                nan],\n",
       "        [0.98782916, 0.99862543, 1.        , 0.6381449 , 0.62092042,\n",
       "         0.62861856],\n",
       "        [0.89714996, 0.69310328, 0.6381449 , 1.        , 0.81348922,\n",
       "         0.87038828],\n",
       "        [0.67675297, 0.51449576, 0.62092042, 0.81348922, 1.        ,\n",
       "         0.33333333],\n",
       "        [0.57263713,        nan, 0.62861856, 0.87038828, 0.33333333,\n",
       "         1.        ]]))"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_cos_sim_matrix_users = AdjustedCosineSimilarityMatrix(table_2_1, 0)\n",
    "adj_cos_sim_matrix_items = AdjustedCosineSimilarityMatrix(table_2_1, 1)\n",
    "adj_cos_sim_matrix_users, adj_cos_sim_matrix_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can rewrite our imputation function to use either the Adjusted-Cosine-Similarity or the Pearson-Correlation-Coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneralizedImputationFlexible(table, threshold, imputation_method: str = \"pearson\"):\n",
    "    table_copy = table.copy()\n",
    "    if imputation_method == \"pearson\":\n",
    "        user_corr_matrix = corr_matrix_func(table_copy, dimension=0)\n",
    "        item_corr_matrix = corr_matrix_func(table_copy, dimension=1)\n",
    "    elif imputation_method == \"adjusted_cosine\":\n",
    "        user_corr_matrix = AdjustedCosineSimilarityMatrix(table, dimension=0)\n",
    "        item_corr_matrix = AdjustedCosineSimilarityMatrix(table, dimension=1)\n",
    "    else:\n",
    "        print(\"Invalid imputation method\")\n",
    "        raise ValueError\n",
    "    # compute means\n",
    "    user_mean = mean_compute(table_copy, dimension=0)\n",
    "    sd_vector_store = sd_vector(table_copy)\n",
    "    # demean the table\n",
    "    table_copy_dm = normalize_table(table_copy)\n",
    "    # for each row in correlation matrix compute the k based on threshold\n",
    "    k = [[] for _ in range(user_corr_matrix.shape[0])]\n",
    "    for i in range(user_corr_matrix.shape[0]):\n",
    "        k[i] = np.where((user_corr_matrix[i] > threshold))[0]\n",
    "    # for each value of k, remove the same number if it is in k, e.g. remove 2 from k[2]\n",
    "    for i in range(user_corr_matrix.shape[0]):\n",
    "        k[i] = np.setdiff1d(k[i], i)\n",
    "    # for each column in correlation matrix compute the k based on threshold\n",
    "    k_cols = [[] for _ in range(item_corr_matrix.shape[1])]\n",
    "    for i in range(item_corr_matrix.shape[1]):\n",
    "        k_cols[i] = np.where((item_corr_matrix[:,i] > threshold))[0]\n",
    "    # for each value of k, remove the same number if it is in k, e.g. remove 2 from k[2]\n",
    "    for i in range(item_corr_matrix.shape[1]):\n",
    "        k_cols[i] = np.setdiff1d(k_cols[i], i)\n",
    "    # for each row and column in the table, perform imputation as follows\n",
    "    # r_ij = user_mean[i] + sd_vector_store[i]*weighted_rating if table[i,j] is missing\n",
    "    # weighted_rating = sum(user_corr_matrix[i][k[i]]*table_copy_dm[k[i],j])/sum(np.abs(user_corr_matrix[i][k[i]])) * (sum(np.abs(user_corr_matrix[i][k[i]])) / (sum(np.abs(user_corr_matrix[i][k[i]])) + sum(np.abs(item_corr_matrix[j][k_cols[j]]))) + sum(item_corr_matrix[j][k_cols[j]]*table_copy_dm[i,k_cols[j]])/sum(np.abs(item_corr_matrix[j][k_cols[j]]))\n",
    "    for i in range(table.shape[0]):\n",
    "        for j in range(table.shape[1]):\n",
    "            if np.isnan(table[i,j]):\n",
    "                weighted_rating = np.sum(user_corr_matrix[i][k[i]]*table_copy_dm[k[i],j])/np.sum(np.abs(user_corr_matrix[i][k[i]])) * (np.sum(np.abs(user_corr_matrix[i][k[i]])) / (np.sum(np.abs(user_corr_matrix[i][k[i]])) + np.sum(np.abs(item_corr_matrix[j][k_cols[j]])))) + np.sum(item_corr_matrix[j][k_cols[j]]*table_copy_dm[i,k_cols[j]])/np.sum(np.abs(item_corr_matrix[j][k_cols[j]])) * (np.sum(np.abs(item_corr_matrix[j][k_cols[j]])) / (np.sum(np.abs(user_corr_matrix[i][k[i]])) + np.sum(np.abs(item_corr_matrix[j][k_cols[j]]))))\n",
    "                table_copy[i,j] = user_mean[i] + sd_vector_store[i]*weighted_rating\n",
    "    return table_copy, corr_matrix, user_mean, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maggo\\AppData\\Local\\Temp\\ipykernel_12308\\2376229201.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  corr = numerator/denominator\n",
      "C:\\Users\\maggo\\AppData\\Local\\Temp\\ipykernel_12308\\3055660411.py:19: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  corr = numerator/denominator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "         4.        ],\n",
       "        [6.        , 7.        , 6.44573888, 4.        , 3.        ,\n",
       "         4.        ],\n",
       "        [2.63396979, 3.        , 3.        , 1.        , 1.        ,\n",
       "         1.07147565],\n",
       "        [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "         4.        ],\n",
       "        [1.        , 1.16295641, 1.        , 2.        , 3.        ,\n",
       "         3.        ]]),\n",
       " array([[7.        , 6.        , 7.        , 4.        , 5.        ,\n",
       "         4.        ],\n",
       "        [6.        , 7.        , 6.44573888, 4.        , 3.        ,\n",
       "         4.        ],\n",
       "        [2.63396979, 3.        , 3.        , 1.        , 1.        ,\n",
       "         1.07147565],\n",
       "        [1.        , 2.        , 2.        , 3.        , 3.        ,\n",
       "         4.        ],\n",
       "        [1.        , 1.16295641, 1.        , 2.        , 3.        ,\n",
       "         3.        ]]))"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_table_2_1_gen_imputed, pearson_corr_matrix_gen, pearson_user_mean_gen, pearson_k_gen = GeneralizedImputationFlexible(table_2_1, 0.7, imputation_method = \"pearson\")\n",
    "cosine_table_2_1_gen_imputed, cosine_corr_matrix_gen, cosine_user_mean_gen, cosine_k_gen = GeneralizedImputationFlexible(table_2_1, 0.7, imputation_method = \"adjusted_cosine\")\n",
    "\n",
    "pearson_table_2_1_gen_imputed, cosine_table_2_1_gen_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "        [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "        [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "        [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "        [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]]),\n",
       " array([[ 1.        ,  0.72347804,  0.89442719, -0.8992288 , -0.82422559],\n",
       "        [ 0.72347804,  1.        ,  0.97072534, -0.72057669, -0.8992288 ],\n",
       "        [ 0.89442719,  0.97072534,  1.        , -1.        , -0.8660254 ],\n",
       "        [-0.8992288 , -0.72057669, -1.        ,  1.        ,  0.87705802],\n",
       "        [-0.82422559, -0.8992288 , -0.8660254 ,  0.87705802,  1.        ]]))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_corr_matrix_gen, cosine_corr_matrix_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  6.,  7.,  4.,  5.,  4.],\n",
       "       [ 6.,  7., nan,  4.,  3.,  4.],\n",
       "       [nan,  3.,  3.,  1.,  1., nan],\n",
       "       [ 1.,  2.,  2.,  3.,  3.,  4.],\n",
       "       [ 1., nan,  1.,  2.,  3.,  3.]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we surprised that these two outcomes are exactly identical? Not too much if we have read correctly footnote 3 on p. 44: Pearson Correlation coefficient is identical to the cosine coefficient for row-wise mean-centered matrices *if* the user-means have been computed using all observed entries (rather than just the mutually observed ones). This is true because in that case the Adjusted-Cosine-similarity that is presented on page 40 is equal to the Pearson-Correlation given at the beginning of this section. What Aggarwal does not mention is that this is *also* true if the user-mean is computed only for the mutually observable set for both Pearson and Adjusted-Cosine. That is because these formulas then boil down to the same mathematical foundations.\n",
    "What Aggarwal (2016) rather means (I stipulate) is that the AdjustedCosine-similarity *as presented for items* boils down to the Pearson-Correlation-Coefficient *for items* if user-means are computed over the entire row. I am hence somewhat at a loss for what Aggarwal actually stipulates here. The mathematical identity of Adjusted-Cosine and Pearson-Correlation should be obvious given that mean-computation is performed in the same fashion. Where it differs is if you compute the similarity between items rather than between users. However, that is nothing new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Other Neighborhood Methods\n",
    "\n",
    "The rest of chapter 2 seems, to me, a high-level overview of other neighborhood-based methods that mainly foreshadow future chapters. As such, it is not fairly straightforward to implement such a diverse set of models if they return later in full length. Therefore, these methods are not coded up here but instead will be coded in the notebooks for the respective chapters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
